{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0rdZqjfNNS22"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7mmArnRQfRO",
        "outputId": "e720f929-da69-483d-8ef2-ac8fa0ff6fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch-geometric in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (1.24.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch-geometric) (5.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch-geometric) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torch-geometric) (3.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->torch-geometric) (0.4.4)\n",
            "Requirement already satisfied: transformers in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.19.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.24.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.4.24)\n",
            "Requirement already satisfied: requests in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maiad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x244a2fc2370>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch_geometric\n",
        "torch_geometric.__version__\n",
        "\n",
        "import random\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from numpy.lib.index_tricks import index_exp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch._C import dtype\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import regex as re\n",
        "import networkx as nx\n",
        "\n",
        "from sklearn.cluster import KMeans, MeanShift\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, adjusted_rand_score\n",
        "from torch_geometric.utils import to_dense_adj, dense_to_sparse, to_edge_index, remove_self_loops\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import KFold,cross_val_predict,cross_val_score,GridSearchCV\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from torch_geometric.nn import GCNConv,GATConv\n",
        "#import torch_sparse\n",
        "from torch import FloatTensor\n",
        "\n",
        "from re import A\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from scipy import sparse as sp\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YHbepGluNS26"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dszt2RUHE7lW"
      },
      "source": [
        "### Dataset class with pygeo graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2R9GQWSvSJEI"
      },
      "outputs": [],
      "source": [
        "class dataset_vibgnn():\n",
        "  def __init__(self,edge_list,edge_weights,n_nodes,classes=None,features = None,verbose = True):\n",
        "\n",
        "    self.n_nodes = n_nodes\n",
        "    self.edge_list = edge_list\n",
        "\n",
        "    if verbose :\n",
        "      print(\"Building the graph...\")\n",
        "\n",
        "    if features is None:\n",
        "      x = torch.tensor(np.identity(self.n_nodes), dtype=torch.float) #maybe in built funciton for py\n",
        "    else:\n",
        "      x = torch.tensor(features, dtype=torch.float)\n",
        "\n",
        "    unzipped_el = list(zip(*edge_list))\n",
        "    edge_index = torch.tensor([unzipped_el[0],unzipped_el[1]], dtype=torch.long)\n",
        "\n",
        "    print('---> Graph')\n",
        "\n",
        "    if classes is not None:\n",
        "      self.graph = Data(x=x,edge_index=edge_index,edge_weights = edge_weights,y = torch.tensor(classes))\n",
        "\n",
        "    else:\n",
        "      self.graph = Data(x=x,edge_index=edge_index)\n",
        "\n",
        "\n",
        "  def build_negatives(self,n=10):\n",
        "    #Dirty : could be optimized\n",
        "\n",
        "    unzipped_el = list(zip(*self.edge_list))\n",
        "    unzipped_el[0] = np.array(unzipped_el[0])\n",
        "    unzipped_el[1] = np.array(unzipped_el[1])\n",
        "\n",
        "    self.pos_examples = []\n",
        "    self.neg_examples = []\n",
        "\n",
        "    nodes_id = np.arange(0,self.n_nodes)\n",
        "\n",
        "\n",
        "\n",
        "    for i in self.edge_list:\n",
        "      self.pos_examples += [(i[0],i[1],1)]\n",
        "\n",
        "      list_nei = unzipped_el[1][unzipped_el[0] == i[0]]\n",
        "      arr = np.delete(nodes_id, list_nei)\n",
        "      indice_negative = np.random.choice(arr, n, replace=False)\n",
        "\n",
        "      self.neg_examples += [(i[0],j,0) for j in indice_negative]\n",
        "  def build_negatives_hetero(self,edge_neg,n = 10):\n",
        "    #Dirty : could be optimized\n",
        "\n",
        "    unzipped_el = list(zip(*self.edge_list))\n",
        "    unzipped_el[0] = np.array(unzipped_el[0])\n",
        "    unzipped_el[1] = np.array(unzipped_el[1])\n",
        "\n",
        "    self.pos_examples = []\n",
        "    self.neg_examples = []\n",
        "\n",
        "    nodes_id = np.arange(0,self.n_nodes)\n",
        "\n",
        "    self.neg_examples = [(i[0],i[1],0) for i in edge_neg]\n",
        "    for bb in range(n):\n",
        "      self.neg_examples += [(i[0],i[1],0) for i in edge_neg]\n",
        "\n",
        "\n",
        "\n",
        "    for i in self.edge_list:\n",
        "      self.pos_examples += [(i[0],i[1],1)]\n",
        "\n",
        "  def build_train(self,edge_tohide=None):\n",
        "    if edge_tohide is not None:\n",
        "      graph_train = copy.deepcopy(self.graph)\n",
        "      edge_list_sub = list(set(self.edge_list)^set(edge_tohide))\n",
        "      unzipped_el = list(zip(*edge_list_sub))\n",
        "      edge_index = torch.tensor([unzipped_el[0],unzipped_el[1]], dtype=torch.long)\n",
        "      graph_train.edge_index = edge_index\n",
        "      return graph_train\n",
        "    else:\n",
        "      return self.graph\n",
        "\n",
        "def build_adjacency_matrix(edges, nb_nodes=None):\n",
        "    if nb_nodes is None:\n",
        "        nb_nodes = np.max(edges) + 1\n",
        "    rows = np.concatenate((edges[:, 0], edges[:, 1]))\n",
        "    cols = np.concatenate((edges[:, 1], edges[:, 0]))\n",
        "    data = np.ones(rows.shape[0], dtype=np.int)\n",
        "    A = sp.csr_matrix((data, (rows, cols)), shape=(nb_nodes, nb_nodes))\n",
        "\n",
        "    assert(A.data.max() == 1)\n",
        "    return A\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9PAmtRHgNS27"
      },
      "source": [
        "### Various Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DhPybVu2Ofyz"
      },
      "outputs": [],
      "source": [
        "def conf_matrix(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    TPR = tp/(tp+fn)\n",
        "    return TPR\n",
        "\n",
        "def eval(model,dataset,x_1_test,x_2_test,y_test):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "\n",
        "    proba_p = model(dataset.graph.x, dataset.graph.edge_index,x_1_test,x_2_test)\n",
        "    y_pred = torch.squeeze(proba_p).numpy()\n",
        "    y_pred_argmax = np.round(y_pred).astype(int)\n",
        "\n",
        "    auc_link=roc_auc_score(y_test, y_pred )\n",
        "\n",
        "  return auc_link\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dwukkwelNS28"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-2Lla7b5NS28"
      },
      "outputs": [],
      "source": [
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, num_features,hidden_channels,heads,c3 = False):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(num_features, hidden_channels[0], heads = heads)  # TODO\n",
        "        self.conv2 = GATConv(hidden_channels[0], hidden_channels[1],heads = heads)  # TODO\n",
        "        self.conv3 = GATConv(hidden_channels[1], hidden_channels[1],heads = heads)  # TODO\n",
        "        self.c3 = c3\n",
        "        self.bn = torch.nn.BatchNorm1d(num_features=hidden_channels[0])\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        if self.c3:\n",
        "          x = x.relu()\n",
        "          x = F.dropout(x, p=0.2, training=self.training)\n",
        "          x = self.conv3(x, edge_index)\n",
        "\n",
        "        return x\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_features,hidden_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = GCNConv(num_features, hidden_channels[0])\n",
        "        self.conv2 = GCNConv(hidden_channels[0], hidden_channels[1])\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x, p=0.2, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "class mlp(torch.nn.Module):\n",
        "        def __init__(self, input_size, output_size):\n",
        "            super(mlp, self).__init__()\n",
        "            self.do = torch.nn.Dropout(p=0.2)\n",
        "            self.fc1 = torch.nn.Linear(input_size, input_size)\n",
        "            self.relu = torch.nn.ReLU()\n",
        "            self.fc2 = torch.nn.Linear(input_size, output_size)\n",
        "            self.tanh = torch.nn.Tanh()\n",
        "            self.fc3 = torch.nn.Linear(input_size, output_size)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.relu(self.fc1(self.do(x)))\n",
        "\n",
        "            x = self.tanh(self.fc2(self.do(x)))\n",
        "\n",
        "            x = self.fc3(self.do(x))\n",
        "            return x\n",
        "\n",
        "class VIB(torch.nn.Module):\n",
        "    def __init__(self, num_features,hidden_channels,encoder = \"GCN\",c3 = False,n_nodes = 1,bibi = 0):\n",
        "        super().__init__()\n",
        "        if encoder == \"GAT\":\n",
        "          self.gnn = GAT(num_features, hidden_channels,heads = 1,c3 = c3)\n",
        "        else:\n",
        "          self.gnn = GCN(num_features, hidden_channels, c3 = c3)\n",
        "          if encoder != \"GCN\":\n",
        "            print(\"unknown encoder, using GCN\")\n",
        "\n",
        "        self.log_a = torch.nn.Parameter(torch.Tensor([0]))\n",
        "\n",
        "        self.b = torch.nn.Parameter(torch.rand(1))\n",
        "\n",
        "        self.mlp_mu = mlp(hidden_channels[1], hidden_channels[1])\n",
        "        self.mlp_logsigma = mlp(hidden_channels[1], hidden_channels[1])\n",
        "\n",
        "        self.N = torch.distributions.Normal(0, 1)\n",
        "\n",
        "        self.S = torch.nn.Parameter(torch.rand(n_nodes,2))\n",
        "\n",
        "        self.M = torch.nn.Parameter(torch.rand(2,hidden_channels[1]))\n",
        "\n",
        "        self.m = torch.nn.ReLU()\n",
        "\n",
        "        self.bibi = bibi\n",
        "\n",
        "\n",
        "    def distance(self,graph,edge_index,x_1,x_2):\n",
        "      embedding = self.gnn(graph, edge_index)\n",
        "\n",
        "\n",
        "      x_1 = torch.squeeze(x_1).long()\n",
        "      x_2 = torch.squeeze(x_2).long()\n",
        "\n",
        "      return torch.sum(torch.square(embedding[x_1] - embedding[x_2]),1)\n",
        "\n",
        "    def forward(self, graph, edge_index,x_1,x_2):\n",
        "\n",
        "        dist = self.distance(graph,edge_index,x_1,x_2)\n",
        "\n",
        "        proba_p = torch.sigmoid( - torch.exp(self.log_a) * dist + self.b)\n",
        "\n",
        "        return proba_p\n",
        "\n",
        "    def loss(self, graph, edge_index,x_1,x_2,y,criterion):\n",
        "\n",
        "        embedding = self.gnn(graph, edge_index)\n",
        "        x_1 = torch.squeeze(x_1).long()\n",
        "        x_2 = torch.squeeze(x_2).long()\n",
        "\n",
        "        dist = torch.sum(torch.square(embedding[x_1] - embedding[x_2]),1)\n",
        "        proba_p = torch.sigmoid( - torch.exp(self.log_a) * dist + self.b)\n",
        "\n",
        "        loss_cl =  criterion(proba_p,torch.squeeze(y))\n",
        "        loss_km = torch.mean(torch.square(embedding - F.softmax(self.S, dim=1) @ self.M))\n",
        "        loss_reg = torch.mean(torch.square(self.M[0,:] - self.M[1,:]))\n",
        "\n",
        "        loss = self.bibi * loss_km + loss_cl #- loss_reg\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def transform_labels(self, labels): # probably a nicer way to do this, but let's roll with it\n",
        "        new_labs = torch.Tensor(len(labels), 2)\n",
        "        for i in range(len(labels)):\n",
        "            if labels[i] == 0:\n",
        "                new_labs[i,0] = 1\n",
        "                new_labs[i,1] = 0\n",
        "            else:\n",
        "                new_labs[i,0] = 0\n",
        "                new_labs[i,1] = 1\n",
        "        return new_labs\n",
        "\n",
        "def visualize(model,dataset,color=None):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    h = model.gnn(dataset.graph.x, dataset.graph.edge_index)\n",
        "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    if color is None:\n",
        "      color = dataset.graph.y\n",
        "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yRMQtIB_NS28"
      },
      "source": [
        "### Encode and Build Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2_0As9gM3Udw"
      },
      "outputs": [],
      "source": [
        "def batch_encode_ant(x, tokenizer,model, device, max_seq_len=128, batch_size=16, return_mask=False):\n",
        "    if type(x) is not list:\n",
        "        x = x.tolist()\n",
        "\n",
        "    z = []\n",
        "\n",
        "    for i in tqdm(range(0, len(x), batch_size)):\n",
        "\n",
        "        bob = tokenizer(x[i: i + batch_size], return_tensors=\"pt\", padding=\"max_length\", max_length=max_seq_len,\n",
        "                        truncation=True)\n",
        "        x_i = bob[\"input_ids\"]\n",
        "        x_m = bob[\"attention_mask\"]\n",
        "\n",
        "        x_i = x_i.to(device)\n",
        "        x_m = x_m.to(device)\n",
        "        z.append(model(x_i,x_m ,output_hidden_states = True)['hidden_states'][-1][:,0,:].detach().cpu().numpy())\n",
        "    z = np.vstack(z)\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2TijUYm9NS29"
      },
      "outputs": [],
      "source": [
        "def preprocess(corpus):\n",
        "  text = []\n",
        "  for txt in corpus:\n",
        "    txt = re.sub(r'(http\\S+)', '', txt, re.I|re.A)\n",
        "    txt = re.sub(r'(@\\S+)', '', txt, re.I|re.A)\n",
        "    txt = re.sub(r'&amp;', ', & ', txt, re.I|re.A)\n",
        "    txt = re.sub(r'&gt;', '>', txt, re.I|re.A)\n",
        "    txt = re.sub(r'\\w(,)\\w', ', ', txt, re.I|re.A)\n",
        "    txt = re.sub(r'\\\\n', '', txt, re.I|re.A)\n",
        "    txt = re.sub(r'\\\\\\'', '\\'', txt, re.I|re.A)\n",
        "    text.append(txt)\n",
        "  return text"
      ]
    },
{
  "cell_type": "code",
  "execution_count": null,
  "metadata": {},
  "outputs": [],
  "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def correlate_clustering(df1, df2, metric_func, clusters_col=\"clusters\", user_col=\"username\", **kwargs):\n",
    "    merged = pd.merge(df1[df1[clusters_col] >= 0], df2[df2[clusters_col] >= 0], on=user_col)\n",
    "    y1, y2 = merged.labels_x, merged.labels_y\n",
    "    return metric_func(y1, y2, **kwargs)\n",
    "\n",
    "def calculate_alignment_matrix(dfs, metric_func, **kwargs):\n",
    "    matrix = np.zeros((len(dfs), len(dfs)))\n",
    "    for i, df1 in tqdm(enumerate(dfs)):\n",
    "        for j, df2 in enumerate(dfs):\n",
    "            matrix[i][j] = correlate_clustering(df1, df2, metric_func, **kwargs)\n",
    "    return matrix\n",
    "\n",
    "def plot_heatmap(frames, topics, func=ami):\n",
    "    hm = calculate_alignment_matrix(frames, func)\n",
    "    hm = pd.DataFrame(hm, columns=topics, index=topics).loc[reversed(topics)]\n",
    "    fig = sns.heatmap(\n",
    "        hm.round(2),\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        annot_kws={\"size\": 30}\n",
    "    )\n",
    "    fig.set_yticklabels(labels=reversed(topics), rotation=45)\n",
    "    fig.set_xticklabels(labels=topics, rotation=45)\n",
    "    n = min(len(topics) * 2, 18)\n",
    "    sns.set(context='notebook', style='white', rc={'figure.figsize': (n, n)}, font_scale=3.5)\n",
    "    return fig\n",
    "\n",
    "def mutual_information(topics, root=\"topicals\"):\n",
    "    frames = list()\n",
    "    for topic in tqdm(topics):\n",
    "        f = np.load(os.path.join(root, f\"/{topic}.npz\"))\n",
    "        users = f[\"users\"]\n",
    "        clusters = f[\"clusters\"]\n",
    "        frames.append(pd.DataFrame({\"users\": users, \"labels\": clusters}))\n",
    "\n",
    "    fig = plot_heatmap(frames, topics)\n"
  ]
}

,
    {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {},
  "outputs": [],
  "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Encoder Class\n",
    "class Encoder:\n",
    "    DEFAULT_MODEL = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "\n",
    "    def __init__(self, model_url: str = DEFAULT_MODEL):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_url: URL of the Universal Sentence Encoder model.\n",
    "        \"\"\"\n",
    "        self.model_url = model_url\n",
    "        self.encoder = self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        return hub.load(self.model_url)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return np.array(self.encoder(text))\n",
    "\n",
    "    def encode_df(self, df: pd.DataFrame, out_path: str, user_col: str = \"username\", text_col: str = \"text\"):\n",
    "        \"\"\"\n",
    "        Generate user-level embeddings and save as .npz.\n",
    "        \"\"\"\n",
    "        users = list()\n",
    "        vectors = list()\n",
    "        counts = list()\n",
    "\n",
    "        for user, tweets in tqdm(df.groupby(user_col)[text_col]):\n",
    "            try:\n",
    "                vs = np.array(self.encoder(tweets.tolist()))\n",
    "                users.append(user)\n",
    "                vectors.append(np.mean(vs, axis=0))\n",
    "                counts.append(len(tweets))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing user {user}: {e}\")\n",
    "\n",
    "        np.savez(out_path, users=np.array(users), vectors=np.array(vectors), counts=np.array(counts),\n",
    "                 allow_pickle=True)\n",
    "\n",
    "# Graph Construction from Embeddings\n",
    "def build_graph_from_embeddings(vectors, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Build graph using cosine similarity as edges.\n",
    "    Args:\n",
    "        vectors: Numpy array of user embeddings.\n",
    "        threshold: Similarity threshold for creating edges.\n",
    "    Returns:\n",
    "        A PyTorch Geometric Data object.\n",
    "    \"\"\"\n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    # Define edges above the threshold\n",
    "    edges = np.argwhere(similarity_matrix > threshold)\n",
    "    edges = edges[edges[:, 0] != edges[:, 1]]  # Remove self-loops\n",
    "\n",
    "    # Create edge_index for PyTorch Geometric\n",
    "    edge_index = torch.tensor(edges.T, dtype=torch.long)\n",
    "\n",
    "    # Create feature matrix (node features)\n",
    "    x = torch.tensor(vectors, dtype=torch.float)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Integration with GUSD Pipeline\n",
    "def run_gusd_pipeline(embedding_file, lr=0.0001, hom=True, use_text=True):\n",
    "    \"\"\"\n",
    "    GUSD pipeline using user-level embeddings and graph construction.\n",
    "    \"\"\"\n",
    "    # Load user-level embeddings\n",
    "    data = np.load(embedding_file, allow_pickle=True)\n",
    "    vectors = data['vectors']\n",
    "    users = data['users']\n",
    "\n",
    "    # Build the graph\n",
    "    graph = build_graph_from_embeddings(vectors)\n",
    "\n",
    "    # Placeholder for classes\n",
    "    classes = np.random.choice([0, 1], size=len(users))  # Simulated binary labels for demo purposes\n",
    "\n",
    "    # Define the model (Assuming VIB exists)\n",
    "    model = VIB(num_features=graph.x.shape[1], hidden_channels=[100, 50], encoder=\"GAT\",\n",
    "                c3=False, n_nodes=len(users), bibi=0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    # KMeans for unsupervised clustering\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(graph.x.numpy())\n",
    "    y_pred = kmeans.labels_\n",
    "    a1 = accuracy_score(classes, y_pred)\n",
    "    a2 = accuracy_score(classes, 1 - y_pred)\n",
    "    acc = max(a1, a2)\n",
    "\n",
    "    print(f\"Initial Accuracy: {acc * 100:.2f}%\")\n",
    "    return graph, model\n",
    "\n",
    "# Run the Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Load a sample dataset (Replace with your dataset)\n",
    "    data = pd.DataFrame({\n",
    "        \"username\": [\"user1\"] * 3 + [\"user2\"] * 3,\n",
    "        \"text\": [\"This is a test tweet\"] * 6\n",
    "    })\n",
    "\n",
    "    # Initialize Encoder and Generate User-Level Embeddings\n",
    "    encoder = Encoder()\n",
    "    encoder.encode_df(data, \"user_embeddings.npz\", user_col=\"username\", text_col=\"text\")\n",
    "\n",
    "    # Run the GUSD pipeline\n",
    "    graph, model = run_gusd_pipeline(\"user_embeddings.npz\")\n"
  ]
}

    ,
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Re3GMf0bNS29"
      },
      "outputs": [],
      "source": [
        "def encode_and_build(txt, data, edge_list_ini, edge_list, edge_weights, inv, hom,use_text,X,lm):\n",
        "\n",
        "    if use_text:\n",
        "        if X is None:\n",
        "          device = torch.device(\"cuda\")\n",
        "\n",
        "          tokenizer = AutoTokenizer.from_pretrained(lm)\n",
        "\n",
        "          model = AutoModelForSequenceClassification.from_pretrained(lm)\n",
        "          model.to(device)\n",
        "\n",
        "          z = batch_encode_ant(txt, tokenizer, model, device, max_seq_len=256, return_mask=True)\n",
        "          data[\"embedding\"] = list(z)\n",
        "\n",
        "          nodes = list(data['id'].unique())\n",
        "          classes = []\n",
        "          X=[]\n",
        "\n",
        "          for i in nodes:\n",
        "              dat = data.loc[data['id'] == i]\n",
        "              X.append(np.mean(dat['embedding']))\n",
        "              classes.append(round(np.mean(dat['label'])) - 1)\n",
        "\n",
        "          X = np.vstack(X)\n",
        "        nodes = list(data['id'].unique())\n",
        "        classes = []\n",
        "        for i in nodes:\n",
        "          dat = data.loc[data['id'] == i]\n",
        "          classes.append(round(np.mean(dat['label'])) - 1)\n",
        "    else:\n",
        "        nodes = list(data['id'].unique())\n",
        "        classes = []\n",
        "        for i in nodes:\n",
        "            dat = data.loc[data['id'] == i]\n",
        "            classes.append(round(np.mean(dat['label'])) - 1)\n",
        "        X = None\n",
        "\n",
        "\n",
        "    # Create an empty graph\n",
        "    graph = nx.Graph()\n",
        "    num_nodes = len(nodes)\n",
        "\n",
        "    edge_list_neg = edge_list\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    graph.add_nodes_from(range(num_nodes))\n",
        "    # Add edges to the graph from the edge list\n",
        "    for edge in edge_list_ini:\n",
        "        source = int(edge[0])\n",
        "        target = int(edge[1])\n",
        "        weight = edge[2]\n",
        "        graph.add_edge(source, target, weight=weight)\n",
        "\n",
        "\n",
        "    self_loops = list(nx.selfloop_edges(graph))\n",
        "    graph.remove_edges_from(self_loops)\n",
        "\n",
        "    if hom:\n",
        "        if inv:\n",
        "            A = nx.adjacency_matrix(graph)\n",
        "            A += A @ A.T\n",
        "            graph = nx.from_numpy_array(A)\n",
        "            self_loops = list(nx.selfloop_edges(graph))\n",
        "            graph.remove_edges_from(self_loops)\n",
        "\n",
        "        df = nx.to_pandas_edgelist(graph)\n",
        "        edge_list = list(zip(list(df['source'].values), list(df['target'].values)))\n",
        "        edge_weights = list(df['weight'].values)\n",
        "    else:\n",
        "        A = nx.adjacency_matrix(graph)\n",
        "        A.data = np.maximum(A.data, 0)\n",
        "        A.data = np.clip(A.data, None, 1)\n",
        "\n",
        "        B = A @ A.T\n",
        "        B.data = np.maximum(B.data, 0)\n",
        "        B.data = np.clip(B.data, None, 1)\n",
        "\n",
        "        A = B - A\n",
        "        A.data = np.maximum(A.data, 0)\n",
        "        A.data = np.clip(A.data, None, 1)\n",
        "\n",
        "        graph = nx.from_numpy_array(A)\n",
        "        self_loops = list(nx.selfloop_edges(graph))\n",
        "        graph.remove_edges_from(self_loops)\n",
        "\n",
        "        df = nx.to_pandas_edgelist(graph)\n",
        "        edge_list = list(zip(list(df['source'].values), list(df['target'].values)))\n",
        "        edge_weights = list(df['weight'].values)\n",
        "\n",
        "    if hom:\n",
        "        dataset = dataset_vibgnn(edge_list, edge_weights, len(classes), classes, X, inv)\n",
        "        dataset.build_negatives(10)\n",
        "    else:\n",
        "        dataset = dataset_vibgnn(edge_list, edge_weights, len(classes), classes, X, inv)\n",
        "        dataset.build_negatives_hetero(edge_list_neg, 10)\n",
        "\n",
        "    full = torch.vstack([torch.Tensor(dataset.neg_examples),torch.Tensor(dataset.pos_examples)])\n",
        "    full = full[torch.randperm(full.shape[0])]\n",
        "\n",
        "    nt = int(full.shape[0] * 0.95)\n",
        "    train = full[0:nt]\n",
        "    test = full[nt:]\n",
        "\n",
        "    x_1_train,x_2_train,y_train = torch.split(train,1,dim = 1)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float)\n",
        "    x_1_test,x_2_test,y_test = torch.split(test,1,dim = 1)\n",
        "\n",
        "    edges_tohide = np.hstack([x_1_test,x_2_test])\n",
        "\n",
        "    edges_tohide = edges_tohide[torch.squeeze(y_test.int()).numpy() == 1].astype(int)\n",
        "    edges_tohide =list(zip(edges_tohide[:,0],edges_tohide[:,1]))\n",
        "    dataset_train = dataset.build_train(edges_tohide)\n",
        "    if X is None:\n",
        "        X = torch.tensor(np.identity(num_nodes), dtype=torch.float)\n",
        "\n",
        "    return dataset, dataset_train, x_1_test, x_2_test, x_1_train, x_2_train, y_test, y_train, classes,X"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iMq9UaFCNS29"
      },
      "source": [
        "### Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OWHfR9ADNS29"
      },
      "outputs": [],
      "source": [
        "def run_model(map_path, edge_path, inv, hom,use_txt,lr,X,lm):\n",
        "    data = pd.read_csv(map_path,delimiter = \"\\t\")\n",
        "    edge_list_ini = np.genfromtxt(edge_path, usecols = [0,1,2])\n",
        "\n",
        "    nodes = list(data['id'].unique())\n",
        "    map = dict(zip(nodes,list(range(len(nodes)))))\n",
        "    edge = []\n",
        "    for i in edge_list_ini:\n",
        "      try:\n",
        "        edge.append((map[i[0]],map[i[1]],i[2]))\n",
        "      except:\n",
        "        bob = 0\n",
        "    edge_list_ini = edge\n",
        "\n",
        "    edge_weights = [i[2] for i in edge_list_ini]\n",
        "    edge_list = [(int(i[0]),int(i[1])) for i in edge_list_ini]\n",
        "\n",
        "    data = data.dropna()\n",
        "    txt = list(data[\"rawTweet\"])\n",
        "    num_nodes = len(nodes)\n",
        "\n",
        "    dataset, dataset_train, x_1_test, x_2_test, x_1_train, x_2_train, y_test, y_train, classes,X = encode_and_build(txt, data, edge_list_ini, edge_list, edge_weights, inv, hom,use_txt,X,lm)\n",
        "\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    model = VIB(num_features = dataset.graph.num_features,hidden_channels=[100,50],encoder = \"GAT\",c3=False,n_nodes = num_nodes,bibi =0)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    acc_p = eval(model,dataset,x_1_test,x_2_test,y_test)\n",
        "\n",
        "    z = model.gnn(dataset.graph.x, dataset.graph.edge_index).detach().numpy()\n",
        "\n",
        "    kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(z)\n",
        "    y_pred = kmeans.labels_\n",
        "\n",
        "    a1 = accuracy_score(classes, y_pred)\n",
        "    a2 = accuracy_score(classes, 1 - y_pred)\n",
        "\n",
        "    accc = max(a1,a2)\n",
        "    acc_rec_final = []\n",
        "    acc_final = []\n",
        "    f_final = []\n",
        "    in_final = []\n",
        "    patience = 10\n",
        "    ref_acc = 0\n",
        "    z_save = None\n",
        "\n",
        "    kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(X)\n",
        "    y_pred = kmeans.labels_\n",
        "\n",
        "    a1 = accuracy_score(classes, y_pred)\n",
        "    a2 = accuracy_score(classes, 1 - y_pred)\n",
        "\n",
        "    accc = max(a1,a2)\n",
        "\n",
        "    print(\"Accuracy text only :\", round(accc, 4) * 100)\n",
        "\n",
        "    if a1 > a2:\n",
        "        best_pred = y_pred\n",
        "    else:\n",
        "        best_pred = 1 - y_pred\n",
        "    _, _, f, _ = precision_recall_fscore_support(classes, best_pred, average='weighted')\n",
        "\n",
        "    print(\"F1 text only :\", round(f, 4) * 100)\n",
        "\n",
        "\n",
        "    for epoch in range(10000):\n",
        "        model.train()\n",
        "\n",
        "        proba_p = model(dataset.graph.x, dataset.graph.edge_index,x_1_test,x_2_test)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "        #loss_vib : vib based loss, loss: soft cont loss without stochasticity\n",
        "\n",
        "        loss = model.loss(dataset_train.x, dataset_train.edge_index,x_1_train,x_2_train,y_train,criterion)\n",
        "        #loss,_,_ = model.loss_vib(dataset_train.x, dataset_train.edge_index,x_1_train,x_2_train,y_train,criterion)\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            model.eval()\n",
        "            acc_p = eval(model,dataset,x_1_test,x_2_test,y_test)\n",
        "            z = model.gnn(dataset.graph.x, dataset.graph.edge_index).detach().numpy()\n",
        "\n",
        "            kmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(z)\n",
        "            if model.bibi == 0:\n",
        "                y_pred = kmeans.labels_\n",
        "            else:\n",
        "                y_pred = torch.argmax(model.S, dim=1).detach().numpy()\n",
        "\n",
        "            a1 = accuracy_score(classes, y_pred)\n",
        "            a2 = accuracy_score(classes, 1 - y_pred)\n",
        "\n",
        "            accc = max(a1,a2)\n",
        "            if a1 > a2:\n",
        "                best_pred = y_pred\n",
        "            else:\n",
        "                best_pred = 1 - y_pred\n",
        "            _, _, f, _ = precision_recall_fscore_support(dataset.graph.y, best_pred, average='weighted')\n",
        "\n",
        "            acc_rec_final.append(acc_p)\n",
        "            acc_final.append(accc)\n",
        "            f_final.append(f)\n",
        "            in_final.append(kmeans.inertia_)\n",
        "            #print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, test AUC (Link prediction): {acc_p:.4f}, Accuracy: {accc:.4f}, F-macro: {f:.4f}, Inertie: {kmeans.inertia_:.4f}')\n",
        "\n",
        "            if acc_p > ref_acc:\n",
        "                patience = 10\n",
        "                ref_acc = acc_p\n",
        "                z_save = z\n",
        "            else:\n",
        "                patience -= 1\n",
        "            if patience == 0:\n",
        "                break\n",
        "\n",
        "    return z_save,acc_rec_final,acc_final,f_final,in_final,X\n"
      ]
    },
{
  "cell_type": "code",
  "execution_count": null,
  "metadata": {},
  "outputs": [],
  "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import hdbscan\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "class Clusterer:\n",
    "    def __init__(self, projection_path):\n",
    "        self.projection_path = projection_path\n",
    "        self._params = self._load_standard_embeddings()\n",
    "        self.N: int = len(self._params[\"users\"])\n",
    "\n",
    "    def _load_standard_embeddings(self):\n",
    "        file = np.load(self.projection_path, allow_pickle=True)\n",
    "        params = dict()\n",
    "        for k in file.keys():\n",
    "            params[k] = file[k]\n",
    "        return params\n",
    "\n",
    "    @staticmethod\n",
    "    def _cluster(standard_embeddings, **kwargs):\n",
    "        return hdbscan.HDBSCAN(**kwargs).fit(standard_embeddings)\n",
    "\n",
    "    def cluster(self, min_samples=None, min_cluster_size=None, \n",
    "                min_samples_divisor=1000, min_cluster_size_divisor=100, \n",
    "                tree_path=None, **kwargs):\n",
    "        if min_samples is None:\n",
    "            kwargs[\"min_samples\"] = max(10, self.N // min_samples_divisor)\n",
    "        if min_cluster_size is None:\n",
    "            kwargs[\"min_cluster_size\"] = max(10, self.N // min_cluster_size_divisor)\n",
    "\n",
    "        model = self._cluster(standard_embeddings=self._params[\"umap\"], **kwargs)\n",
    "        self._params[\"clusters\"] = model.labels_\n",
    "        np.savez(open(self.projection_path, 'wb'), **self._params)\n",
    "        if tree_path is not None:\n",
    "            pickle.dump(model.condensed_tree_, open(tree_path, 'wb'), protocol=3)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_tree(path):\n",
    "        sns.set(context='notebook', style='white', rc={'figure.figsize': (15, 10)})\n",
    "        return pickle.load(open(path, 'rb')).plot()\n",
    "\n",
    "    def plot(self, labels_col=\"clusters\"):\n",
    "        return Projector.plot(embeddings=self._params[\"umap\"], labels=self._params[labels_col])\n",
    "\n",
    "    def inject_labels(self, users, labels):\n",
    "        labels_dict = dict(zip(users, labels))\n",
    "        self._params[\"labels\"] = np.array(\n",
    "            [labels_dict[u] if u in labels_dict else 'unk' for u in self._params[\"users\"]]\n",
    "        )\n",
    "\n",
    "    def align_clusters_with_labels(self, allow_multiple_clusters=True):\n",
    "        labels = self._params[\"labels\"]\n",
    "        ind = labels != 'unk'\n",
    "        users = self._params[\"users\"][ind]\n",
    "        labels = labels[ind]\n",
    "\n",
    "        df = pd.DataFrame({\"username\": users, \"labels\": labels}).merge(\n",
    "            pd.DataFrame({\"username\": self._params[\"users\"], \"clusters\": self._params[\"clusters\"]})\n",
    "        )\n",
    "\n",
    "        g = df.groupby([\"label\", \"clusters\"]).count().sort_values(\"username\", ascending=False)\n",
    "        d = {}\n",
    "        while len(g) > 0:\n",
    "            label, cluster = g.index[0]\n",
    "            d[cluster] = label\n",
    "            g = g.reset_index()\n",
    "            g = g[(g.label != label) & (g.clusters != cluster)].set_index([\"label\", \"clusters\"]).sort_values(\"username\", ascending=False)\n",
    "\n",
    "        unlabeled_clusters = set(df.clusters) - set(d.keys())\n",
    "        if allow_multiple_clusters and len(unlabeled_clusters) > 0:\n",
    "            g = df.groupby([\"label\", \"clusters\"]).count().sort_values(\"username\", ascending=False).reset_index()\n",
    "            for c in unlabeled_clusters:\n",
    "                l = g.set_index(\"clusters\").loc[c].label\n",
    "                if isinstance(l, pd.Series):\n",
    "                    l = l.iloc[0]\n",
    "                d[c] = l\n",
    "                g = g[g.clusters != c]\n",
    "\n",
    "        self._params[\"predictions\"] = np.array([d[x] if x in d else 'unk' for x in self._params['clusters']])\n",
    "\n",
    "    def evaluate(self, metric=f1_score, report=True):\n",
    "        if \"predictions\" not in self._params:\n",
    "            raise Exception(\"No labels aligned with clusters\")\n",
    "\n",
    "        y = self._params[\"labels\"]\n",
    "        p = self._params[\"predictions\"]\n",
    "\n",
    "        ind = y != 'unk'\n",
    "        y = y[ind]\n",
    "        p = p[ind]\n",
    "\n",
    "        s = set(y)\n",
    "        if report:\n",
    "            return pd.DataFrame(classification_report(y, p, labels=s, output_dict=True))\n",
    "\n",
    "        return metric(y, p, labels=s, average='micro')\n",
    "\n",
    "    @staticmethod\n",
    "    def cluster_projection_grid_search(trials_dir, users=None, labels=None, allow_multiple_clusters=True):\n",
    "        results = dict()\n",
    "        for fn in tqdm(os.listdir(trials_dir)):\n",
    "            if not fn.endswith(\"npz\"):\n",
    "                continue\n",
    "            min_dist, n_neighbors = fn.replace(\".npz\", '').split(\"_\")\n",
    "            projection_path = os.path.join(trials_dir, fn)\n",
    "            c = Clusterer(projection_path)\n",
    "            c.cluster()\n",
    "            plot_path = os.path.join(trials_dir, f\"{min_dist}_{n_neighbors}.png\")\n",
    "            c.inject_labels(users=users, labels=labels)\n",
    "            c.align_clusters_with_labels(allow_multiple_clusters=allow_multiple_clusters)\n",
    "            fig = c.plot()\n",
    "            plt.savefig(plot_path, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            score = c.evaluate()\n",
    "            results.setdefault(min_dist, dict())\n",
    "            results[min_dist][n_neighbors] = score\n",
    "        return results\n"
  ]
}

  ,
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ek1QpzVC9EjW"
      },
      "source": [
        "## Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kCfbWGWNS2-",
        "outputId": "dd3af78c-7776-4eac-d48a-67da67b589e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---> Trial 1 of 10 :\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 226kB [00:00, 16.9MB/s]\n",
            "Downloading: 455kB [00:00, 8.00MB/s]\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 150/150 [01:07<00:00,  2.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---> Graph\n",
            "Accuracy text only : 77.53\n",
            "F1 text only : 68.26\n",
            "Using text : True \tHomophily : True \tLearning rate : 0.0001\n",
            "Reconstruction accuracy : 84.36 \tInertia : 29.07\n",
            "---> Trial 2 of 10 :\n",
            "---> Graph\n",
            "Accuracy text only : 77.53\n",
            "F1 text only : 68.26\n",
            "Using text : True \tHomophily : True \tLearning rate : 0.0001\n",
            "Reconstruction accuracy : 74.74 \tInertia : 35.39\n",
            "---> Trial 3 of 10 :\n",
            "---> Graph\n",
            "Accuracy text only : 77.53\n",
            "F1 text only : 68.26\n",
            "Using text : True \tHomophily : True \tLearning rate : 0.0001\n",
            "Reconstruction accuracy : 71.91 \tInertia : 29.83\n",
            "---> Trial 4 of 10 :\n",
            "---> Graph\n",
            "Accuracy text only : 77.53\n",
            "F1 text only : 68.26\n",
            "Using text : True \tHomophily : True \tLearning rate : 0.0001\n",
            "Reconstruction accuracy : 88.57000000000001 \tInertia : 38.42\n",
            "---> Trial 5 of 10 :\n",
            "---> Graph\n",
            "Accuracy text only : 77.53\n",
            "F1 text only : 68.26\n",
            "Using text : True \tHomophily : True \tLearning rate : 0.0001\n",
            "Reconstruction accuracy : 73.36 \tInertia : 32.67\n",
            "---> Trial 6 of 10 :\n",
            "---> Graph\n",
            "Accuracy text only : 77.53\n",
            "F1 text only : 68.26\n",
            "Using text : True \tHomophily : True \tLearning rate : 0.0001\n",
            "Reconstruction accuracy : 78.27 \tInertia : 36.81\n",
            "---> Trial 7 of 10 :\n",
            "---> Graph\n",
            "Accuracy text only : 77.53\n",
            "F1 text only : 68.26\n",
            "Using text : True \tHomophily : True \tLearning rate : 0.0001\n",
            "Reconstruction accuracy : 79.56 \tInertia : 36.18\n",
            "---> Trial 8 of 10 :\n",
            "---> Graph\n",
            "Accuracy text only : 77.53\n",
            "F1 text only : 68.26\n",
            "Using text : True \tHomophily : True \tLearning rate : 0.0001\n",
            "Reconstruction accuracy : 84.37 \tInertia : 30.04\n",
            "---> Trial 9 of 10 :\n",
            "---> Graph\n",
            "Accuracy text only : 77.53\n",
            "F1 text only : 68.26\n",
            "Using text : True \tHomophily : True \tLearning rate : 0.0001\n",
            "Reconstruction accuracy : 83.86 \tInertia : 28.96\n",
            "---> Trial 10 of 10 :\n",
            "---> Graph\n",
            "Accuracy text only : 77.53\n",
            "F1 text only : 68.26\n",
            "Using text : True \tHomophily : True \tLearning rate : 0.0001\n",
            "Reconstruction accuracy : 78.14999999999999 \tInertia : 35.59\n",
            "\n",
            "Dataset : conref\n",
            "Average accuracy : 70.28 \tStandard deviation : 2.1399999999999997\n",
            "Average weighted F1 : 72.16 \tStandard deviation : 1.77\n"
          ]
        }
      ],
      "source": [
        "# options : 'euro', 'timme', 'cd', 'conref'\n",
        "dat = 'conref'\n",
        "# topics for CD; options : 'all', 'abortion', 'marijuana', 'gayRights', or 'obama'\n",
        "top = 'abortion'\n",
        "# whether or not to use TIMME-All when running with TIMME; False runs TIMME-Pure\n",
        "t_all = True\n",
        "path = './Processed/'\n",
        "\n",
        "lm = \"distilbert-base-uncased\"\n",
        "\n",
        "if dat == 'cd':\n",
        "  dat = dat + top\n",
        "\n",
        "if dat == 'timme':\n",
        "  if t_all:\n",
        "    dat = dat + '_all'\n",
        "\n",
        "map_path = path + dat + '_mapping.csv'\n",
        "edge_path = path + dat + '_graph.txt'\n",
        "\n",
        "X = None\n",
        "# whether or not to use text embeddings\n",
        "use_text = True\n",
        "# whether or not to use the homophilic version; False runs the heterophilic version\n",
        "hom = True\n",
        "lr = 0.0001\n",
        "# number of trials to run\n",
        "max_t = 10\n",
        "\n",
        "best_acc = []\n",
        "best_f1 = []\n",
        "for i in range(max_t):\n",
        "  print(\"---> Trial\", i + 1, \"of\", max_t, \":\")\n",
        "  emb,acc_rec_final,acc_final,f_final,in_final,X = run_model(map_path, edge_path, False, hom,use_text,lr,X,lm)\n",
        "  print(\"Using text :\",use_text,\"\\tHomophily :\",hom,\"\\tLearning rate :\",lr)\n",
        "  print(\"Reconstruction accuracy :\",round(acc_rec_final[-5],4) * 100,\"\\tInertia :\",round(in_final[-5], 2))\n",
        "  i = np.argmin(in_final[-10:])\n",
        "\n",
        "  best_acc.append(acc_final[-10:][i])\n",
        "  best_f1.append(f_final[-10:][i])\n",
        "\n",
        "if dat == 'timme':\n",
        "  if not t_all:\n",
        "    dat = dat + '_pure'\n",
        "print(\"\\nDataset :\",dat)\n",
        "if max_t > 1:\n",
        "  print(\"Average accuracy :\",round(np.mean(best_acc),4) * 100,\"\\tStandard deviation :\",round(np.sqrt(np.var(best_acc)),4) * 100)\n",
        "  print(\"Average weighted F1 :\",round(np.mean(best_f1),4) * 100,\"\\tStandard deviation :\",round(np.sqrt(np.var(best_f1)),4) * 100)\n",
        "else:\n",
        "  print(\"Accuracy :\",round(best_acc) * 100)\n",
        "  print(\"Weighted F1 :\",round(best_f1) * 100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "c4b978b7b08878481fd7e451f24ddb719a2a4d190b5461eb2af65cf267689d03"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
